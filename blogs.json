{"status":"ok","feed":{"url":"https://medium.com/feed/@Michael_Ram","title":"Stories by Michael Ramirez on Medium","link":"https://medium.com/@Michael_Ram?source=rss-a89c00df1203------2","author":"","description":"Stories by Michael Ramirez on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*L5jv3lrMLq8wtDR14_Hrag.png"},"items":[{"title":"Ethics in AI #1","pubDate":"2024-10-30 06:02:35","link":"https://medium.com/@Michael_Ram/ethics-in-ai-1-6bccc89006c6?source=rss-a89c00df1203------2","guid":"https://medium.com/p/6bccc89006c6","author":"Michael Ramirez","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/369/1*90uSuHnZWvtlMLe16HVnhA.png\"></figure><p>Ethics in AI is something that I am very passionate about as its still a rising subject in the latest era of generative AI and data privacy legislation. As more research is being done in deep learning and our models are becoming much more powerful than ever before, we come to a wall of \u201cwhat is the line?\u201d. As we build the latest model that exceeds our expectations, it may be obvious to just push it out into production or to show it off to your peers, but there should always be a momentary pause before we do so. We are at a moment in time where our models can cause serious harm, even if we assume the greatest intentions with\u00a0them.</p>\n<p>Let\u2019s take for example: Policing with AI. The AI models that came out to help society are actually crumbling to disaster with negative feedback loops. Policing with AI has taken a turn for the worse as with the latest in crime statistics, we see that even though black and white Americans use marijuana at the same rate, black Americans get arrested 3.3x more often than white Americans. It seems that this isn\u2019t a case of black Americans committing the crime more often, but police being stationed and predicting future policing, NOT crime. It is a clearer picture when we can visualize that if we have police on the ground due to inherit racial bias against black Americans and make arrests, we begin our feedback loop. They see crime in black neighborhoods, they then send more police to perform more arrests. As there are more police in that area, they will make much more arrests, and as they make more arrests, more police get sent to that area due to the data shown. This data is then trained on our models which create a very negative feedback loop that trusts the data, but doesn\u2019t look at the overall context in the situation. (<a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC9170008/\">https://pmc.ncbi.nlm.nih.gov/articles/PMC9170008/</a>)</p>\n<p>We then have the problem of how this data is collected. Are we collecting data ethically and given the proper permissions or adequate payments to collect the data? As we have seen with OpenAI and Google\u2019s models, they are under heat with the law due to privacy laws and how they are collecting their data. One such case would be Authors Guild v. Google. This case opened an entire can of worms on data collection and the ways massive corporations can steal or abuse privacy agreements for our data. (<a href=\"https://graphicartistsguild.org/oracle-vs-google-guild-joins-amicus-brief/\">https://graphicartistsguild.org/oracle-vs-google-guild-joins-amicus-brief/</a>)</p>\n<p>This will be one post of many to help me understand and share the critical effects of acknowledging the potential disasters that can occur within each new model that is built. What is the model going to be used for? How was the data collected? Is the data collected ethically? Will there be manual edits to the model to prevent feedback loops? What else can we think of that may harm others with these\u00a0results?</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6bccc89006c6\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/369/1*90uSuHnZWvtlMLe16HVnhA.png\"></figure><p>Ethics in AI is something that I am very passionate about as its still a rising subject in the latest era of generative AI and data privacy legislation. As more research is being done in deep learning and our models are becoming much more powerful than ever before, we come to a wall of \u201cwhat is the line?\u201d. As we build the latest model that exceeds our expectations, it may be obvious to just push it out into production or to show it off to your peers, but there should always be a momentary pause before we do so. We are at a moment in time where our models can cause serious harm, even if we assume the greatest intentions with\u00a0them.</p>\n<p>Let\u2019s take for example: Policing with AI. The AI models that came out to help society are actually crumbling to disaster with negative feedback loops. Policing with AI has taken a turn for the worse as with the latest in crime statistics, we see that even though black and white Americans use marijuana at the same rate, black Americans get arrested 3.3x more often than white Americans. It seems that this isn\u2019t a case of black Americans committing the crime more often, but police being stationed and predicting future policing, NOT crime. It is a clearer picture when we can visualize that if we have police on the ground due to inherit racial bias against black Americans and make arrests, we begin our feedback loop. They see crime in black neighborhoods, they then send more police to perform more arrests. As there are more police in that area, they will make much more arrests, and as they make more arrests, more police get sent to that area due to the data shown. This data is then trained on our models which create a very negative feedback loop that trusts the data, but doesn\u2019t look at the overall context in the situation. (<a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC9170008/\">https://pmc.ncbi.nlm.nih.gov/articles/PMC9170008/</a>)</p>\n<p>We then have the problem of how this data is collected. Are we collecting data ethically and given the proper permissions or adequate payments to collect the data? As we have seen with OpenAI and Google\u2019s models, they are under heat with the law due to privacy laws and how they are collecting their data. One such case would be Authors Guild v. Google. This case opened an entire can of worms on data collection and the ways massive corporations can steal or abuse privacy agreements for our data. (<a href=\"https://graphicartistsguild.org/oracle-vs-google-guild-joins-amicus-brief/\">https://graphicartistsguild.org/oracle-vs-google-guild-joins-amicus-brief/</a>)</p>\n<p>This will be one post of many to help me understand and share the critical effects of acknowledging the potential disasters that can occur within each new model that is built. What is the model going to be used for? How was the data collected? Is the data collected ethically? Will there be manual edits to the model to prevent feedback loops? What else can we think of that may harm others with these\u00a0results?</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6bccc89006c6\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","ai","ethics-in-tech"]}]}